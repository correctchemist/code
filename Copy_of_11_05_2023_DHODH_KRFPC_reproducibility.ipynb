{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/correctchemist/code/blob/main/Copy_of_11_05_2023_DHODH_KRFPC_reproducibility.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to your dataset file\n",
        "dataset_file = \"/content/dhodh vae.csv\"\n",
        "\n",
        "# Read the dataset into a Pandas DataFrame\n",
        "df = pd.read_csv(dataset_file)\n",
        "\n",
        "# Print the first few rows of the DataFrame to verify the data is properly loaded\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7p8xQXouGZ-",
        "outputId": "f62ebf32-9214-4e04-a621-c069e4986051"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0 molecule_chembl_id                                      SMILES  \\\n",
            "0           0       CHEMBL199572  CN(C(=O)c1ccc(-c2ccccc2)cc1)c1ccccc1C(=O)O   \n",
            "1           1       CHEMBL199574      O=C(Nc1ccccc1C(=O)O)c1ccc2cc(Br)ccc2c1   \n",
            "2           2       CHEMBL372561   CN(C(=O)c1ccc2cc(Br)ccc2c1)c1ccccc1C(=O)O   \n",
            "3           3       CHEMBL370865     O=C(Nc1ccccc1C(=O)O)c1ccc(-c2ccccc2)cc1   \n",
            "4           4       CHEMBL199575       CN(C(=O)c1ccc2ccccc2c1)c1ccccc1C(=O)O   \n",
            "\n",
            "      pIC50  \n",
            "0  4.370590  \n",
            "1  3.845880  \n",
            "2  4.029653  \n",
            "3  3.813892  \n",
            "4  3.698970  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to your dataset file\n",
        "dataset_file = \"/content/dhodh vae.csv\"\n",
        "\n",
        "# Read the dataset into a Pandas DataFrame\n",
        "df = pd.read_csv('/content/dhodh vae.csv')\n",
        "\n",
        "# Perform data cleaning operations\n",
        "\n",
        "# Drop any rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Remove any leading or trailing whitespace from the SMILES strings\n",
        "df['SMILES'] = df['SMILES'].str.strip()\n",
        "\n",
        "# Convert the bioactivity column to numeric if necessary\n",
        "df['pIC50'] = pd.to_numeric(df['pIC50'], errors='coerce')\n",
        "\n",
        "# Drop any rows with invalid or non-numeric bioactivity values\n",
        "df = df.dropna(subset=['pIC50'])\n",
        "\n",
        "# Remove duplicates, if any\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Reset the index after data cleaning\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Print the cleaned dataset\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4fMM7AbwDwT",
        "outputId": "2ce49a8b-5a36-4bfc-9bcf-9794926efd8d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0 molecule_chembl_id                                      SMILES  \\\n",
            "0           0       CHEMBL199572  CN(C(=O)c1ccc(-c2ccccc2)cc1)c1ccccc1C(=O)O   \n",
            "1           1       CHEMBL199574      O=C(Nc1ccccc1C(=O)O)c1ccc2cc(Br)ccc2c1   \n",
            "2           2       CHEMBL372561   CN(C(=O)c1ccc2cc(Br)ccc2c1)c1ccccc1C(=O)O   \n",
            "3           3       CHEMBL370865     O=C(Nc1ccccc1C(=O)O)c1ccc(-c2ccccc2)cc1   \n",
            "4           4       CHEMBL199575       CN(C(=O)c1ccc2ccccc2c1)c1ccccc1C(=O)O   \n",
            "\n",
            "      pIC50  \n",
            "0  4.370590  \n",
            "1  3.845880  \n",
            "2  4.029653  \n",
            "3  3.813892  \n",
            "4  3.698970  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_xeOOwQl8uG",
        "outputId": "d70d75ef-9dc4-4f72-eb89-2e96d91ac411"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the path to your dataset file\n",
        "dataset_file = \"/content/dhodh vae.csv\"\n",
        "\n",
        "# Read the dataset into a Pandas DataFrame\n",
        "df = pd.read_csv(dataset_file)\n",
        "\n",
        "# Perform data cleaning operations (if necessary)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reset the index for the training and testing sets\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "# Print the shapes of the training and testing sets to verify the split\n",
        "print(\"Training set shape:\", train_df.shape)\n",
        "print(\"Testing set shape:\", test_df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqhBkzMaxbxm",
        "outputId": "2d3125b5-0f00-4480-f153-18a6083db9da"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (372, 4)\n",
            "Testing set shape: (93, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rdkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw7NupGdfOle",
        "outputId": "913687f9-0b77-4a96-f972-a6a1d168b138"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.22.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (8.4.0)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2023.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "\n",
        "# Define the path to your dataset file\n",
        "dataset_file = \"/content/dhodh vae.csv\"\n",
        "\n",
        "# Read the dataset into a Pandas DataFrame\n",
        "df = pd.read_csv(dataset_file)\n",
        "\n",
        "# Preprocess SMILES strings\n",
        "def preprocess_smiles(smiles):\n",
        "    # Convert SMILES string to RDKit Mol object\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    \n",
        "    if mol is None:\n",
        "        # Handle invalid SMILES strings\n",
        "        return None\n",
        "    \n",
        "    # Perform any additional preprocessing steps here\n",
        "    \n",
        "    # Convert Mol object back to SMILES string\n",
        "    preprocessed_smiles = Chem.MolToSmiles(mol)\n",
        "    \n",
        "    return preprocessed_smiles\n",
        "\n",
        "# Apply SMILES preprocessing to the DataFrame\n",
        "df['Preprocessed_SMILES'] = df['SMILES'].apply(preprocess_smiles)\n",
        "\n",
        "# Remove rows with invalid SMILES strings (if any)\n",
        "df = df.dropna(subset=['Preprocessed_SMILES'])\n",
        "\n",
        "# Save the preprocessed SMILES to a file\n",
        "preprocessed_file = \"/content/preprocessed_smiles.csv\"\n",
        "df[['Preprocessed_SMILES']].to_csv(preprocessed_file, index=False)\n",
        "\n",
        "# Print the first few rows of the DataFrame to verify the preprocessing\n",
        "if __name__ == '__main__':\n",
        "    print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vYjxJ_zm9pd",
        "outputId": "67588c6a-0c9e-45ff-a510-e77c89369703"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0 molecule_chembl_id                                      SMILES  \\\n",
            "0           0       CHEMBL199572  CN(C(=O)c1ccc(-c2ccccc2)cc1)c1ccccc1C(=O)O   \n",
            "1           1       CHEMBL199574      O=C(Nc1ccccc1C(=O)O)c1ccc2cc(Br)ccc2c1   \n",
            "2           2       CHEMBL372561   CN(C(=O)c1ccc2cc(Br)ccc2c1)c1ccccc1C(=O)O   \n",
            "3           3       CHEMBL370865     O=C(Nc1ccccc1C(=O)O)c1ccc(-c2ccccc2)cc1   \n",
            "4           4       CHEMBL199575       CN(C(=O)c1ccc2ccccc2c1)c1ccccc1C(=O)O   \n",
            "\n",
            "      pIC50                         Preprocessed_SMILES  \n",
            "0  4.370590  CN(C(=O)c1ccc(-c2ccccc2)cc1)c1ccccc1C(=O)O  \n",
            "1  3.845880      O=C(Nc1ccccc1C(=O)O)c1ccc2cc(Br)ccc2c1  \n",
            "2  4.029653   CN(C(=O)c1ccc2cc(Br)ccc2c1)c1ccccc1C(=O)O  \n",
            "3  3.813892     O=C(Nc1ccccc1C(=O)O)c1ccc(-c2ccccc2)cc1  \n",
            "4  3.698970       CN(C(=O)c1ccc2ccccc2c1)c1ccccc1C(=O)O  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to your dataset file\n",
        "dataset_file = \"/content/dhodh vae.csv\"\n",
        "\n",
        "# Read the dataset into a Pandas DataFrame\n",
        "df = pd.read_csv(dataset_file)\n",
        "\n",
        "# Preprocess bioactivity data\n",
        "def preprocess_bioactivity(pIC50):\n",
        "    # Perform any preprocessing steps here\n",
        "    \n",
        "    # Convert to desired format or scale\n",
        "    preprocessed_pIC50 = ...  # Replace with your preprocessing logic\n",
        "    \n",
        "    return preprocessed_pIC50\n",
        "\n",
        "# Apply bioactivity preprocessing to the DataFrame\n",
        "df['Preprocessed_pIC50'] = df['pIC50'].apply(preprocess_bioactivity)\n",
        "\n",
        "# Save the preprocessed bioactivity data to a file\n",
        "preprocessed_file = \"/content/preprocessed_bioactivity.csv\"\n",
        "df[['Preprocessed_pIC50']].to_csv(preprocessed_file, index=False)\n",
        "\n",
        "# Print the first few rows of the DataFrame to verify the preprocessing\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzOJVrBkzXJE",
        "outputId": "f18bec30-f5f9-4c0b-ff8f-f22e506ecf41"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0 molecule_chembl_id                                      SMILES  \\\n",
            "0           0       CHEMBL199572  CN(C(=O)c1ccc(-c2ccccc2)cc1)c1ccccc1C(=O)O   \n",
            "1           1       CHEMBL199574      O=C(Nc1ccccc1C(=O)O)c1ccc2cc(Br)ccc2c1   \n",
            "2           2       CHEMBL372561   CN(C(=O)c1ccc2cc(Br)ccc2c1)c1ccccc1C(=O)O   \n",
            "3           3       CHEMBL370865     O=C(Nc1ccccc1C(=O)O)c1ccc(-c2ccccc2)cc1   \n",
            "4           4       CHEMBL199575       CN(C(=O)c1ccc2ccccc2c1)c1ccccc1C(=O)O   \n",
            "\n",
            "      pIC50 Preprocessed_pIC50  \n",
            "0  4.370590           Ellipsis  \n",
            "1  3.845880           Ellipsis  \n",
            "2  4.029653           Ellipsis  \n",
            "3  3.813892           Ellipsis  \n",
            "4  3.698970           Ellipsis  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to your dataset file\n",
        "dataset_file = \"/content/dhodh vae.csv\"\n",
        "\n",
        "# Read the dataset into a Pandas DataFrame\n",
        "df = pd.read_csv(dataset_file)\n",
        "\n",
        "# Preprocess bioactivity data\n",
        "def preprocess_bioactivity(pIC50):\n",
        "    # Example preprocessing: converting pIC50 to IC50 (inverse transformation)\n",
        "    IC50 = 10 ** (-pIC50)\n",
        "    \n",
        "    return IC50\n",
        "\n",
        "# Apply bioactivity preprocessing to the DataFrame\n",
        "df['Preprocessed_pIC50'] = df['pIC50'].apply(preprocess_bioactivity)\n",
        "\n",
        "# Save the preprocessed bioactivity data to a file\n",
        "preprocessed_file = \"/content/preprocessed_bioactivity.csv\"\n",
        "df[['Preprocessed_pIC50']].to_csv(preprocessed_file, index=False)\n",
        "\n",
        "# Print the first few rows of the DataFrame to verify the preprocessing\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwfc9H_e1ZiC",
        "outputId": "5c2d37be-0d55-427c-e257-f1e1b7851396"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0 molecule_chembl_id                                      SMILES  \\\n",
            "0           0       CHEMBL199572  CN(C(=O)c1ccc(-c2ccccc2)cc1)c1ccccc1C(=O)O   \n",
            "1           1       CHEMBL199574      O=C(Nc1ccccc1C(=O)O)c1ccc2cc(Br)ccc2c1   \n",
            "2           2       CHEMBL372561   CN(C(=O)c1ccc2cc(Br)ccc2c1)c1ccccc1C(=O)O   \n",
            "3           3       CHEMBL370865     O=C(Nc1ccccc1C(=O)O)c1ccc(-c2ccccc2)cc1   \n",
            "4           4       CHEMBL199575       CN(C(=O)c1ccc2ccccc2c1)c1ccccc1C(=O)O   \n",
            "\n",
            "      pIC50  Preprocessed_pIC50  \n",
            "0  4.370590            0.000043  \n",
            "1  3.845880            0.000143  \n",
            "2  4.029653            0.000093  \n",
            "3  3.813892            0.000154  \n",
            "4  3.698970            0.000200  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into input (X) and output (y)\n",
        "df= pd.read_csv('/content/preprocessed_smiles.csv')\n",
        "X = df['Preprocessed_SMILES']\n",
        "df= pd.read_csv('/content/preprocessed_bioactivity.csv')\n",
        "y = df['Preprocessed_pIC50']\n",
        "\n",
        "# Print the shape of the input and output data\n",
        "print(\"Input shape:\", X.shape)\n",
        "print(\"Output shape:\", y.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf9Z7rDE3LXa",
        "outputId": "4f64c821-d1f4-44e3-d27f-ad3bcaec73e8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (465,)\n",
            "Output shape: (465,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shape of the training and validation sets\n",
        "print(\"Training set shapes:\")\n",
        "print(\"Input shape:\", X_train.shape)\n",
        "print(\"Output shape:\", y_train.shape)\n",
        "print()\n",
        "print(\"Validation set shapes:\")\n",
        "print(\"Input shape:\", X_val.shape)\n",
        "print(\"Output shape:\", y_val.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FvlCGDy7hpu",
        "outputId": "934eb0c2-e153-4344-8705-0d17b07d0e95"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shapes:\n",
            "Input shape: (372,)\n",
            "Output shape: (372,)\n",
            "\n",
            "Validation set shapes:\n",
            "Input shape: (93,)\n",
            "Output shape: (93,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert the list to a NumPy array\n",
        "X = np.array(X)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shape of the training and validation sets\n",
        "print(\"Training set shapes:\")\n",
        "print(\"Input shape:\", X_train.shape)\n",
        "print(\"Output shape:\", y_train.shape)\n",
        "print()\n",
        "print(\"Validation set shapes:\")\n",
        "print(\"Input shape:\", X_val.shape)\n",
        "print(\"Output shape:\", y_val.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHsr7rLENjND",
        "outputId": "176852ef-301f-432c-c525-3b9f77019737"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shapes:\n",
            "Input shape: (372,)\n",
            "Output shape: (372,)\n",
            "\n",
            "Validation set shapes:\n",
            "Input shape: (93,)\n",
            "Output shape: (93,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sgzWUKt9Z-I",
        "outputId": "d5c88155-88ec-48a0-ab2f-53193d1df8a3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pickle\n",
        "\n",
        "# Load the preprocessed SMILES data\n",
        "df = pd.read_csv('/content/preprocessed_smiles.csv')\n",
        "X = df['Preprocessed_SMILES']\n",
        "\n",
        "# Convert SMILES strings to list of characters\n",
        "smiles = X.tolist()\n",
        "characters = list(set(''.join(smiles)))\n",
        "\n",
        "# Create a dictionary mapping characters to their corresponding indices\n",
        "character_to_index = {character: index for index, character in enumerate(characters)}\n",
        "\n",
        "# Create a OneHotEncoder object\n",
        "onehot_encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
        "\n",
        "# Fit the OneHotEncoder object to the list of characters\n",
        "encoded_characters = onehot_encoder.fit_transform(np.array(characters).reshape(-1, 1))\n",
        "\n",
        "# Create a dictionary mapping characters to their corresponding one-hot encoding\n",
        "character_to_encoding = {character: encoded_characters[index] for index, character in enumerate(characters)}\n",
        "\n",
        "# Convert SMILES strings to one-hot encoded representation\n",
        "encoded_smiles = np.array([\n",
        "    [character_to_encoding[character] for character in smiles_string]\n",
        "    for smiles_string in smiles\n",
        "], dtype=object)\n",
        "\n",
        "# Save the character-to-encoding dictionary and the encoded SMILES to files\n",
        "with open('/content/character_to_encoding.pkl', 'wb') as f:\n",
        "    pickle.dump(character_to_encoding, f)\n",
        "\n",
        "np.save('/content/encoded_smiles.npy', encoded_smiles)\n"
      ],
      "metadata": {
        "id": "mIj8eb07BUlF"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the preprocessed data\n",
        "encoded_smiles = np.load('/content/encoded_smiles.npy', allow_pickle=True)\n",
        "# ... other code for loading character_to_encoding dictionary\n",
        "\n",
        "# Print the length and shape of each element in encoded_smiles\n",
        "for i, item in enumerate(encoded_smiles):\n",
        "    print(f\"Item {i}: Length={len(item)}, Shape={np.array(item).shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUaxXtwI3Go5",
        "outputId": "9dc7de9e-cd41-421e-f128-b08637ce2973"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Item 0: Length=42, Shape=(42, 31)\n",
            "Item 1: Length=38, Shape=(38, 31)\n",
            "Item 2: Length=41, Shape=(41, 31)\n",
            "Item 3: Length=39, Shape=(39, 31)\n",
            "Item 4: Length=37, Shape=(37, 31)\n",
            "Item 5: Length=34, Shape=(34, 31)\n",
            "Item 6: Length=41, Shape=(41, 31)\n",
            "Item 7: Length=38, Shape=(38, 31)\n",
            "Item 8: Length=45, Shape=(45, 31)\n",
            "Item 9: Length=33, Shape=(33, 31)\n",
            "Item 10: Length=41, Shape=(41, 31)\n",
            "Item 11: Length=38, Shape=(38, 31)\n",
            "Item 12: Length=41, Shape=(41, 31)\n",
            "Item 13: Length=38, Shape=(38, 31)\n",
            "Item 14: Length=38, Shape=(38, 31)\n",
            "Item 15: Length=31, Shape=(31, 31)\n",
            "Item 16: Length=33, Shape=(33, 31)\n",
            "Item 17: Length=38, Shape=(38, 31)\n",
            "Item 18: Length=42, Shape=(42, 31)\n",
            "Item 19: Length=27, Shape=(27, 31)\n",
            "Item 20: Length=26, Shape=(26, 31)\n",
            "Item 21: Length=33, Shape=(33, 31)\n",
            "Item 22: Length=28, Shape=(28, 31)\n",
            "Item 23: Length=35, Shape=(35, 31)\n",
            "Item 24: Length=28, Shape=(28, 31)\n",
            "Item 25: Length=29, Shape=(29, 31)\n",
            "Item 26: Length=35, Shape=(35, 31)\n",
            "Item 27: Length=29, Shape=(29, 31)\n",
            "Item 28: Length=29, Shape=(29, 31)\n",
            "Item 29: Length=29, Shape=(29, 31)\n",
            "Item 30: Length=32, Shape=(32, 31)\n",
            "Item 31: Length=32, Shape=(32, 31)\n",
            "Item 32: Length=32, Shape=(32, 31)\n",
            "Item 33: Length=35, Shape=(35, 31)\n",
            "Item 34: Length=38, Shape=(38, 31)\n",
            "Item 35: Length=31, Shape=(31, 31)\n",
            "Item 36: Length=45, Shape=(45, 31)\n",
            "Item 37: Length=39, Shape=(39, 31)\n",
            "Item 38: Length=39, Shape=(39, 31)\n",
            "Item 39: Length=31, Shape=(31, 31)\n",
            "Item 40: Length=31, Shape=(31, 31)\n",
            "Item 41: Length=38, Shape=(38, 31)\n",
            "Item 42: Length=26, Shape=(26, 31)\n",
            "Item 43: Length=31, Shape=(31, 31)\n",
            "Item 44: Length=37, Shape=(37, 31)\n",
            "Item 45: Length=25, Shape=(25, 31)\n",
            "Item 46: Length=31, Shape=(31, 31)\n",
            "Item 47: Length=32, Shape=(32, 31)\n",
            "Item 48: Length=33, Shape=(33, 31)\n",
            "Item 49: Length=42, Shape=(42, 31)\n",
            "Item 50: Length=32, Shape=(32, 31)\n",
            "Item 51: Length=32, Shape=(32, 31)\n",
            "Item 52: Length=38, Shape=(38, 31)\n",
            "Item 53: Length=56, Shape=(56, 31)\n",
            "Item 54: Length=65, Shape=(65, 31)\n",
            "Item 55: Length=27, Shape=(27, 31)\n",
            "Item 56: Length=63, Shape=(63, 31)\n",
            "Item 57: Length=28, Shape=(28, 31)\n",
            "Item 58: Length=31, Shape=(31, 31)\n",
            "Item 59: Length=88, Shape=(88, 31)\n",
            "Item 60: Length=44, Shape=(44, 31)\n",
            "Item 61: Length=41, Shape=(41, 31)\n",
            "Item 62: Length=41, Shape=(41, 31)\n",
            "Item 63: Length=45, Shape=(45, 31)\n",
            "Item 64: Length=47, Shape=(47, 31)\n",
            "Item 65: Length=45, Shape=(45, 31)\n",
            "Item 66: Length=51, Shape=(51, 31)\n",
            "Item 67: Length=45, Shape=(45, 31)\n",
            "Item 68: Length=47, Shape=(47, 31)\n",
            "Item 69: Length=43, Shape=(43, 31)\n",
            "Item 70: Length=39, Shape=(39, 31)\n",
            "Item 71: Length=42, Shape=(42, 31)\n",
            "Item 72: Length=32, Shape=(32, 31)\n",
            "Item 73: Length=33, Shape=(33, 31)\n",
            "Item 74: Length=41, Shape=(41, 31)\n",
            "Item 75: Length=45, Shape=(45, 31)\n",
            "Item 76: Length=32, Shape=(32, 31)\n",
            "Item 77: Length=38, Shape=(38, 31)\n",
            "Item 78: Length=37, Shape=(37, 31)\n",
            "Item 79: Length=36, Shape=(36, 31)\n",
            "Item 80: Length=36, Shape=(36, 31)\n",
            "Item 81: Length=35, Shape=(35, 31)\n",
            "Item 82: Length=32, Shape=(32, 31)\n",
            "Item 83: Length=41, Shape=(41, 31)\n",
            "Item 84: Length=36, Shape=(36, 31)\n",
            "Item 85: Length=30, Shape=(30, 31)\n",
            "Item 86: Length=32, Shape=(32, 31)\n",
            "Item 87: Length=35, Shape=(35, 31)\n",
            "Item 88: Length=32, Shape=(32, 31)\n",
            "Item 89: Length=35, Shape=(35, 31)\n",
            "Item 90: Length=34, Shape=(34, 31)\n",
            "Item 91: Length=32, Shape=(32, 31)\n",
            "Item 92: Length=31, Shape=(31, 31)\n",
            "Item 93: Length=30, Shape=(30, 31)\n",
            "Item 94: Length=29, Shape=(29, 31)\n",
            "Item 95: Length=30, Shape=(30, 31)\n",
            "Item 96: Length=43, Shape=(43, 31)\n",
            "Item 97: Length=30, Shape=(30, 31)\n",
            "Item 98: Length=31, Shape=(31, 31)\n",
            "Item 99: Length=33, Shape=(33, 31)\n",
            "Item 100: Length=34, Shape=(34, 31)\n",
            "Item 101: Length=35, Shape=(35, 31)\n",
            "Item 102: Length=27, Shape=(27, 31)\n",
            "Item 103: Length=28, Shape=(28, 31)\n",
            "Item 104: Length=31, Shape=(31, 31)\n",
            "Item 105: Length=22, Shape=(22, 31)\n",
            "Item 106: Length=23, Shape=(23, 31)\n",
            "Item 107: Length=24, Shape=(24, 31)\n",
            "Item 108: Length=25, Shape=(25, 31)\n",
            "Item 109: Length=26, Shape=(26, 31)\n",
            "Item 110: Length=27, Shape=(27, 31)\n",
            "Item 111: Length=26, Shape=(26, 31)\n",
            "Item 112: Length=26, Shape=(26, 31)\n",
            "Item 113: Length=29, Shape=(29, 31)\n",
            "Item 114: Length=36, Shape=(36, 31)\n",
            "Item 115: Length=36, Shape=(36, 31)\n",
            "Item 116: Length=36, Shape=(36, 31)\n",
            "Item 117: Length=31, Shape=(31, 31)\n",
            "Item 118: Length=40, Shape=(40, 31)\n",
            "Item 119: Length=34, Shape=(34, 31)\n",
            "Item 120: Length=34, Shape=(34, 31)\n",
            "Item 121: Length=33, Shape=(33, 31)\n",
            "Item 122: Length=37, Shape=(37, 31)\n",
            "Item 123: Length=35, Shape=(35, 31)\n",
            "Item 124: Length=37, Shape=(37, 31)\n",
            "Item 125: Length=34, Shape=(34, 31)\n",
            "Item 126: Length=33, Shape=(33, 31)\n",
            "Item 127: Length=35, Shape=(35, 31)\n",
            "Item 128: Length=36, Shape=(36, 31)\n",
            "Item 129: Length=36, Shape=(36, 31)\n",
            "Item 130: Length=35, Shape=(35, 31)\n",
            "Item 131: Length=41, Shape=(41, 31)\n",
            "Item 132: Length=35, Shape=(35, 31)\n",
            "Item 133: Length=51, Shape=(51, 31)\n",
            "Item 134: Length=45, Shape=(45, 31)\n",
            "Item 135: Length=39, Shape=(39, 31)\n",
            "Item 136: Length=39, Shape=(39, 31)\n",
            "Item 137: Length=48, Shape=(48, 31)\n",
            "Item 138: Length=42, Shape=(42, 31)\n",
            "Item 139: Length=36, Shape=(36, 31)\n",
            "Item 140: Length=36, Shape=(36, 31)\n",
            "Item 141: Length=36, Shape=(36, 31)\n",
            "Item 142: Length=46, Shape=(46, 31)\n",
            "Item 143: Length=40, Shape=(40, 31)\n",
            "Item 144: Length=34, Shape=(34, 31)\n",
            "Item 145: Length=34, Shape=(34, 31)\n",
            "Item 146: Length=33, Shape=(33, 31)\n",
            "Item 147: Length=33, Shape=(33, 31)\n",
            "Item 148: Length=53, Shape=(53, 31)\n",
            "Item 149: Length=47, Shape=(47, 31)\n",
            "Item 150: Length=41, Shape=(41, 31)\n",
            "Item 151: Length=41, Shape=(41, 31)\n",
            "Item 152: Length=45, Shape=(45, 31)\n",
            "Item 153: Length=39, Shape=(39, 31)\n",
            "Item 154: Length=33, Shape=(33, 31)\n",
            "Item 155: Length=33, Shape=(33, 31)\n",
            "Item 156: Length=35, Shape=(35, 31)\n",
            "Item 157: Length=35, Shape=(35, 31)\n",
            "Item 158: Length=46, Shape=(46, 31)\n",
            "Item 159: Length=40, Shape=(40, 31)\n",
            "Item 160: Length=51, Shape=(51, 31)\n",
            "Item 161: Length=45, Shape=(45, 31)\n",
            "Item 162: Length=52, Shape=(52, 31)\n",
            "Item 163: Length=46, Shape=(46, 31)\n",
            "Item 164: Length=40, Shape=(40, 31)\n",
            "Item 165: Length=40, Shape=(40, 31)\n",
            "Item 166: Length=51, Shape=(51, 31)\n",
            "Item 167: Length=45, Shape=(45, 31)\n",
            "Item 168: Length=39, Shape=(39, 31)\n",
            "Item 169: Length=39, Shape=(39, 31)\n",
            "Item 170: Length=37, Shape=(37, 31)\n",
            "Item 171: Length=37, Shape=(37, 31)\n",
            "Item 172: Length=48, Shape=(48, 31)\n",
            "Item 173: Length=42, Shape=(42, 31)\n",
            "Item 174: Length=36, Shape=(36, 31)\n",
            "Item 175: Length=36, Shape=(36, 31)\n",
            "Item 176: Length=49, Shape=(49, 31)\n",
            "Item 177: Length=43, Shape=(43, 31)\n",
            "Item 178: Length=51, Shape=(51, 31)\n",
            "Item 179: Length=45, Shape=(45, 31)\n",
            "Item 180: Length=49, Shape=(49, 31)\n",
            "Item 181: Length=37, Shape=(37, 31)\n",
            "Item 182: Length=37, Shape=(37, 31)\n",
            "Item 183: Length=48, Shape=(48, 31)\n",
            "Item 184: Length=42, Shape=(42, 31)\n",
            "Item 185: Length=36, Shape=(36, 31)\n",
            "Item 186: Length=36, Shape=(36, 31)\n",
            "Item 187: Length=45, Shape=(45, 31)\n",
            "Item 188: Length=39, Shape=(39, 31)\n",
            "Item 189: Length=33, Shape=(33, 31)\n",
            "Item 190: Length=33, Shape=(33, 31)\n",
            "Item 191: Length=29, Shape=(29, 31)\n",
            "Item 192: Length=31, Shape=(31, 31)\n",
            "Item 193: Length=32, Shape=(32, 31)\n",
            "Item 194: Length=34, Shape=(34, 31)\n",
            "Item 195: Length=34, Shape=(34, 31)\n",
            "Item 196: Length=35, Shape=(35, 31)\n",
            "Item 197: Length=33, Shape=(33, 31)\n",
            "Item 198: Length=33, Shape=(33, 31)\n",
            "Item 199: Length=35, Shape=(35, 31)\n",
            "Item 200: Length=36, Shape=(36, 31)\n",
            "Item 201: Length=37, Shape=(37, 31)\n",
            "Item 202: Length=41, Shape=(41, 31)\n",
            "Item 203: Length=44, Shape=(44, 31)\n",
            "Item 204: Length=38, Shape=(38, 31)\n",
            "Item 205: Length=40, Shape=(40, 31)\n",
            "Item 206: Length=40, Shape=(40, 31)\n",
            "Item 207: Length=40, Shape=(40, 31)\n",
            "Item 208: Length=46, Shape=(46, 31)\n",
            "Item 209: Length=46, Shape=(46, 31)\n",
            "Item 210: Length=45, Shape=(45, 31)\n",
            "Item 211: Length=45, Shape=(45, 31)\n",
            "Item 212: Length=39, Shape=(39, 31)\n",
            "Item 213: Length=43, Shape=(43, 31)\n",
            "Item 214: Length=41, Shape=(41, 31)\n",
            "Item 215: Length=38, Shape=(38, 31)\n",
            "Item 216: Length=38, Shape=(38, 31)\n",
            "Item 217: Length=39, Shape=(39, 31)\n",
            "Item 218: Length=39, Shape=(39, 31)\n",
            "Item 219: Length=39, Shape=(39, 31)\n",
            "Item 220: Length=40, Shape=(40, 31)\n",
            "Item 221: Length=42, Shape=(42, 31)\n",
            "Item 222: Length=43, Shape=(43, 31)\n",
            "Item 223: Length=40, Shape=(40, 31)\n",
            "Item 224: Length=42, Shape=(42, 31)\n",
            "Item 225: Length=41, Shape=(41, 31)\n",
            "Item 226: Length=40, Shape=(40, 31)\n",
            "Item 227: Length=44, Shape=(44, 31)\n",
            "Item 228: Length=37, Shape=(37, 31)\n",
            "Item 229: Length=34, Shape=(34, 31)\n",
            "Item 230: Length=37, Shape=(37, 31)\n",
            "Item 231: Length=38, Shape=(38, 31)\n",
            "Item 232: Length=38, Shape=(38, 31)\n",
            "Item 233: Length=36, Shape=(36, 31)\n",
            "Item 234: Length=39, Shape=(39, 31)\n",
            "Item 235: Length=35, Shape=(35, 31)\n",
            "Item 236: Length=33, Shape=(33, 31)\n",
            "Item 237: Length=34, Shape=(34, 31)\n",
            "Item 238: Length=34, Shape=(34, 31)\n",
            "Item 239: Length=31, Shape=(31, 31)\n",
            "Item 240: Length=34, Shape=(34, 31)\n",
            "Item 241: Length=34, Shape=(34, 31)\n",
            "Item 242: Length=31, Shape=(31, 31)\n",
            "Item 243: Length=31, Shape=(31, 31)\n",
            "Item 244: Length=40, Shape=(40, 31)\n",
            "Item 245: Length=30, Shape=(30, 31)\n",
            "Item 246: Length=52, Shape=(52, 31)\n",
            "Item 247: Length=56, Shape=(56, 31)\n",
            "Item 248: Length=49, Shape=(49, 31)\n",
            "Item 249: Length=45, Shape=(45, 31)\n",
            "Item 250: Length=32, Shape=(32, 31)\n",
            "Item 251: Length=38, Shape=(38, 31)\n",
            "Item 252: Length=37, Shape=(37, 31)\n",
            "Item 253: Length=29, Shape=(29, 31)\n",
            "Item 254: Length=36, Shape=(36, 31)\n",
            "Item 255: Length=34, Shape=(34, 31)\n",
            "Item 256: Length=35, Shape=(35, 31)\n",
            "Item 257: Length=35, Shape=(35, 31)\n",
            "Item 258: Length=29, Shape=(29, 31)\n",
            "Item 259: Length=41, Shape=(41, 31)\n",
            "Item 260: Length=33, Shape=(33, 31)\n",
            "Item 261: Length=33, Shape=(33, 31)\n",
            "Item 262: Length=32, Shape=(32, 31)\n",
            "Item 263: Length=31, Shape=(31, 31)\n",
            "Item 264: Length=33, Shape=(33, 31)\n",
            "Item 265: Length=37, Shape=(37, 31)\n",
            "Item 266: Length=46, Shape=(46, 31)\n",
            "Item 267: Length=35, Shape=(35, 31)\n",
            "Item 268: Length=33, Shape=(33, 31)\n",
            "Item 269: Length=31, Shape=(31, 31)\n",
            "Item 270: Length=37, Shape=(37, 31)\n",
            "Item 271: Length=40, Shape=(40, 31)\n",
            "Item 272: Length=45, Shape=(45, 31)\n",
            "Item 273: Length=43, Shape=(43, 31)\n",
            "Item 274: Length=43, Shape=(43, 31)\n",
            "Item 275: Length=44, Shape=(44, 31)\n",
            "Item 276: Length=41, Shape=(41, 31)\n",
            "Item 277: Length=29, Shape=(29, 31)\n",
            "Item 278: Length=35, Shape=(35, 31)\n",
            "Item 279: Length=34, Shape=(34, 31)\n",
            "Item 280: Length=39, Shape=(39, 31)\n",
            "Item 281: Length=45, Shape=(45, 31)\n",
            "Item 282: Length=36, Shape=(36, 31)\n",
            "Item 283: Length=37, Shape=(37, 31)\n",
            "Item 284: Length=35, Shape=(35, 31)\n",
            "Item 285: Length=35, Shape=(35, 31)\n",
            "Item 286: Length=35, Shape=(35, 31)\n",
            "Item 287: Length=39, Shape=(39, 31)\n",
            "Item 288: Length=38, Shape=(38, 31)\n",
            "Item 289: Length=37, Shape=(37, 31)\n",
            "Item 290: Length=36, Shape=(36, 31)\n",
            "Item 291: Length=35, Shape=(35, 31)\n",
            "Item 292: Length=37, Shape=(37, 31)\n",
            "Item 293: Length=36, Shape=(36, 31)\n",
            "Item 294: Length=34, Shape=(34, 31)\n",
            "Item 295: Length=38, Shape=(38, 31)\n",
            "Item 296: Length=35, Shape=(35, 31)\n",
            "Item 297: Length=33, Shape=(33, 31)\n",
            "Item 298: Length=33, Shape=(33, 31)\n",
            "Item 299: Length=25, Shape=(25, 31)\n",
            "Item 300: Length=44, Shape=(44, 31)\n",
            "Item 301: Length=41, Shape=(41, 31)\n",
            "Item 302: Length=36, Shape=(36, 31)\n",
            "Item 303: Length=35, Shape=(35, 31)\n",
            "Item 304: Length=35, Shape=(35, 31)\n",
            "Item 305: Length=37, Shape=(37, 31)\n",
            "Item 306: Length=34, Shape=(34, 31)\n",
            "Item 307: Length=35, Shape=(35, 31)\n",
            "Item 308: Length=35, Shape=(35, 31)\n",
            "Item 309: Length=39, Shape=(39, 31)\n",
            "Item 310: Length=43, Shape=(43, 31)\n",
            "Item 311: Length=39, Shape=(39, 31)\n",
            "Item 312: Length=45, Shape=(45, 31)\n",
            "Item 313: Length=39, Shape=(39, 31)\n",
            "Item 314: Length=29, Shape=(29, 31)\n",
            "Item 315: Length=35, Shape=(35, 31)\n",
            "Item 316: Length=39, Shape=(39, 31)\n",
            "Item 317: Length=39, Shape=(39, 31)\n",
            "Item 318: Length=39, Shape=(39, 31)\n",
            "Item 319: Length=38, Shape=(38, 31)\n",
            "Item 320: Length=38, Shape=(38, 31)\n",
            "Item 321: Length=40, Shape=(40, 31)\n",
            "Item 322: Length=39, Shape=(39, 31)\n",
            "Item 323: Length=41, Shape=(41, 31)\n",
            "Item 324: Length=41, Shape=(41, 31)\n",
            "Item 325: Length=39, Shape=(39, 31)\n",
            "Item 326: Length=39, Shape=(39, 31)\n",
            "Item 327: Length=39, Shape=(39, 31)\n",
            "Item 328: Length=42, Shape=(42, 31)\n",
            "Item 329: Length=51, Shape=(51, 31)\n",
            "Item 330: Length=51, Shape=(51, 31)\n",
            "Item 331: Length=48, Shape=(48, 31)\n",
            "Item 332: Length=25, Shape=(25, 31)\n",
            "Item 333: Length=29, Shape=(29, 31)\n",
            "Item 334: Length=35, Shape=(35, 31)\n",
            "Item 335: Length=31, Shape=(31, 31)\n",
            "Item 336: Length=39, Shape=(39, 31)\n",
            "Item 337: Length=31, Shape=(31, 31)\n",
            "Item 338: Length=37, Shape=(37, 31)\n",
            "Item 339: Length=33, Shape=(33, 31)\n",
            "Item 340: Length=33, Shape=(33, 31)\n",
            "Item 341: Length=34, Shape=(34, 31)\n",
            "Item 342: Length=33, Shape=(33, 31)\n",
            "Item 343: Length=33, Shape=(33, 31)\n",
            "Item 344: Length=41, Shape=(41, 31)\n",
            "Item 345: Length=43, Shape=(43, 31)\n",
            "Item 346: Length=51, Shape=(51, 31)\n",
            "Item 347: Length=46, Shape=(46, 31)\n",
            "Item 348: Length=54, Shape=(54, 31)\n",
            "Item 349: Length=37, Shape=(37, 31)\n",
            "Item 350: Length=37, Shape=(37, 31)\n",
            "Item 351: Length=38, Shape=(38, 31)\n",
            "Item 352: Length=40, Shape=(40, 31)\n",
            "Item 353: Length=40, Shape=(40, 31)\n",
            "Item 354: Length=41, Shape=(41, 31)\n",
            "Item 355: Length=41, Shape=(41, 31)\n",
            "Item 356: Length=41, Shape=(41, 31)\n",
            "Item 357: Length=42, Shape=(42, 31)\n",
            "Item 358: Length=35, Shape=(35, 31)\n",
            "Item 359: Length=39, Shape=(39, 31)\n",
            "Item 360: Length=39, Shape=(39, 31)\n",
            "Item 361: Length=45, Shape=(45, 31)\n",
            "Item 362: Length=45, Shape=(45, 31)\n",
            "Item 363: Length=49, Shape=(49, 31)\n",
            "Item 364: Length=49, Shape=(49, 31)\n",
            "Item 365: Length=43, Shape=(43, 31)\n",
            "Item 366: Length=43, Shape=(43, 31)\n",
            "Item 367: Length=40, Shape=(40, 31)\n",
            "Item 368: Length=42, Shape=(42, 31)\n",
            "Item 369: Length=42, Shape=(42, 31)\n",
            "Item 370: Length=43, Shape=(43, 31)\n",
            "Item 371: Length=41, Shape=(41, 31)\n",
            "Item 372: Length=47, Shape=(47, 31)\n",
            "Item 373: Length=46, Shape=(46, 31)\n",
            "Item 374: Length=50, Shape=(50, 31)\n",
            "Item 375: Length=47, Shape=(47, 31)\n",
            "Item 376: Length=51, Shape=(51, 31)\n",
            "Item 377: Length=54, Shape=(54, 31)\n",
            "Item 378: Length=50, Shape=(50, 31)\n",
            "Item 379: Length=51, Shape=(51, 31)\n",
            "Item 380: Length=56, Shape=(56, 31)\n",
            "Item 381: Length=26, Shape=(26, 31)\n",
            "Item 382: Length=42, Shape=(42, 31)\n",
            "Item 383: Length=31, Shape=(31, 31)\n",
            "Item 384: Length=33, Shape=(33, 31)\n",
            "Item 385: Length=31, Shape=(31, 31)\n",
            "Item 386: Length=43, Shape=(43, 31)\n",
            "Item 387: Length=39, Shape=(39, 31)\n",
            "Item 388: Length=46, Shape=(46, 31)\n",
            "Item 389: Length=39, Shape=(39, 31)\n",
            "Item 390: Length=36, Shape=(36, 31)\n",
            "Item 391: Length=35, Shape=(35, 31)\n",
            "Item 392: Length=32, Shape=(32, 31)\n",
            "Item 393: Length=29, Shape=(29, 31)\n",
            "Item 394: Length=46, Shape=(46, 31)\n",
            "Item 395: Length=32, Shape=(32, 31)\n",
            "Item 396: Length=33, Shape=(33, 31)\n",
            "Item 397: Length=35, Shape=(35, 31)\n",
            "Item 398: Length=26, Shape=(26, 31)\n",
            "Item 399: Length=40, Shape=(40, 31)\n",
            "Item 400: Length=34, Shape=(34, 31)\n",
            "Item 401: Length=41, Shape=(41, 31)\n",
            "Item 402: Length=41, Shape=(41, 31)\n",
            "Item 403: Length=36, Shape=(36, 31)\n",
            "Item 404: Length=34, Shape=(34, 31)\n",
            "Item 405: Length=37, Shape=(37, 31)\n",
            "Item 406: Length=35, Shape=(35, 31)\n",
            "Item 407: Length=26, Shape=(26, 31)\n",
            "Item 408: Length=45, Shape=(45, 31)\n",
            "Item 409: Length=37, Shape=(37, 31)\n",
            "Item 410: Length=44, Shape=(44, 31)\n",
            "Item 411: Length=34, Shape=(34, 31)\n",
            "Item 412: Length=34, Shape=(34, 31)\n",
            "Item 413: Length=39, Shape=(39, 31)\n",
            "Item 414: Length=34, Shape=(34, 31)\n",
            "Item 415: Length=37, Shape=(37, 31)\n",
            "Item 416: Length=40, Shape=(40, 31)\n",
            "Item 417: Length=38, Shape=(38, 31)\n",
            "Item 418: Length=34, Shape=(34, 31)\n",
            "Item 419: Length=34, Shape=(34, 31)\n",
            "Item 420: Length=34, Shape=(34, 31)\n",
            "Item 421: Length=38, Shape=(38, 31)\n",
            "Item 422: Length=37, Shape=(37, 31)\n",
            "Item 423: Length=37, Shape=(37, 31)\n",
            "Item 424: Length=35, Shape=(35, 31)\n",
            "Item 425: Length=36, Shape=(36, 31)\n",
            "Item 426: Length=36, Shape=(36, 31)\n",
            "Item 427: Length=38, Shape=(38, 31)\n",
            "Item 428: Length=32, Shape=(32, 31)\n",
            "Item 429: Length=38, Shape=(38, 31)\n",
            "Item 430: Length=35, Shape=(35, 31)\n",
            "Item 431: Length=26, Shape=(26, 31)\n",
            "Item 432: Length=34, Shape=(34, 31)\n",
            "Item 433: Length=43, Shape=(43, 31)\n",
            "Item 434: Length=38, Shape=(38, 31)\n",
            "Item 435: Length=26, Shape=(26, 31)\n",
            "Item 436: Length=31, Shape=(31, 31)\n",
            "Item 437: Length=37, Shape=(37, 31)\n",
            "Item 438: Length=25, Shape=(25, 31)\n",
            "Item 439: Length=35, Shape=(35, 31)\n",
            "Item 440: Length=41, Shape=(41, 31)\n",
            "Item 441: Length=31, Shape=(31, 31)\n",
            "Item 442: Length=38, Shape=(38, 31)\n",
            "Item 443: Length=38, Shape=(38, 31)\n",
            "Item 444: Length=43, Shape=(43, 31)\n",
            "Item 445: Length=41, Shape=(41, 31)\n",
            "Item 446: Length=35, Shape=(35, 31)\n",
            "Item 447: Length=32, Shape=(32, 31)\n",
            "Item 448: Length=39, Shape=(39, 31)\n",
            "Item 449: Length=37, Shape=(37, 31)\n",
            "Item 450: Length=34, Shape=(34, 31)\n",
            "Item 451: Length=34, Shape=(34, 31)\n",
            "Item 452: Length=23, Shape=(23, 31)\n",
            "Item 453: Length=39, Shape=(39, 31)\n",
            "Item 454: Length=40, Shape=(40, 31)\n",
            "Item 455: Length=29, Shape=(29, 31)\n",
            "Item 456: Length=29, Shape=(29, 31)\n",
            "Item 457: Length=32, Shape=(32, 31)\n",
            "Item 458: Length=29, Shape=(29, 31)\n",
            "Item 459: Length=31, Shape=(31, 31)\n",
            "Item 460: Length=31, Shape=(31, 31)\n",
            "Item 461: Length=35, Shape=(35, 31)\n",
            "Item 462: Length=38, Shape=(38, 31)\n",
            "Item 463: Length=29, Shape=(29, 31)\n",
            "Item 464: Length=54, Shape=(54, 31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==2.8.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAsttxMyKcN5",
        "outputId": "3f46ebea-9eb1-40fc-958b-a88f51ee68d9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.8.0\n",
            "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (23.3.3)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.8.0)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8.0)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (16.0.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.14.1)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8.0)\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8.0)\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8.0)\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.32.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.54.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.17.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.1)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n",
            "Installing collected packages: tf-estimator-nightly, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.0\n",
            "    Uninstalling tensorboard-data-server-0.7.0:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.2\n",
            "    Uninstalling tensorboard-2.12.2:\n",
            "      Successfully uninstalled tensorboard-2.12.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import mse, binary_crossentropy\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Load the preprocessed data\n",
        "encoded_smiles = np.load('/content/encoded_smiles.npy', allow_pickle=True)\n",
        "\n",
        "# Reshape the data to be a 2D array\n",
        "encoded_smiles = encoded_smiles.reshape(-1, input_dim)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val = train_test_split(encoded_smiles, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the dimensionality of the latent space\n",
        "latent_dim = 10\n",
        "\n",
        "# Define the input shape\n",
        "input_dim = encoded_smiles.shape[-1]\n",
        "\n",
        "# Define the encoder architecture\n",
        "inputs = Input(shape=(input_dim,), name='encoder_input')\n",
        "x = Dense(256, activation='relu')(inputs)\n",
        "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "# Define the sampling layer\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.0, stddev=1.0)\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = Lambda(sampling, name='z')([z_mean, z_log_var])\n",
        "\n",
        "# Define the decoder architecture\n",
        "decoder_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
        "x = Dense(256, activation='relu')(decoder_inputs)\n",
        "outputs = Dense(input_dim, activation='sigmoid')(x)\n",
        "\n",
        "# Define the encoder and decoder models\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "decoder = Model(decoder_inputs, outputs, name='decoder')\n",
        "\n",
        "# Define the VAE model\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, outputs, name='vae')\n",
        "\n",
        "# Define the reconstruction loss\n",
        "reconstruction_loss = binary_crossentropy(inputs, outputs) * input_dim\n",
        "\n",
        "# Define the KL divergence loss\n",
        "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "\n",
        "# Define the total loss\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "# Compile the VAE model\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer='adam')\n",
        "\n",
        "# Convert NumPy arrays to tensors\n",
        "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "X_val = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
        "\n",
        "# Train the VAE model\n",
        "epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "history = vae.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, X_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "aTIL9TADBYTz",
        "outputId": "d640bae8-8644-415a-df1a-dbe9e20c2436"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-96947683b129>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_smiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Define the dimensionality of the latent space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": False})\n",
        "vae.save('vae_model', save_format='tf')"
      ],
      "metadata": {
        "id": "A8ZoaeQSY-3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def untraced_function(func):\n",
        "  @tf.function\n",
        "  def wrapped_function(*args, **kwargs):\n",
        "    return func(*args, **kwargs)\n",
        "\n",
        "  return wrapped_function\n",
        "\n",
        "\n",
        "@untraced_function\n",
        "def my_untraced_function():\n",
        "  # Do something untraced\n",
        "  print(\"This is untraced!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OfBeQC9KaUDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading my model"
      ],
      "metadata": {
        "id": "Al_UvapjbJgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vae = tf.keras.models.load_model(\"vae_model.h5\")\n",
        "\n",
        "my_untraced_function()\n"
      ],
      "metadata": {
        "id": "WFbNdzV7a7TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the VAE model\n",
        "vae = tf.keras.models.load_model(\"vae_model.h5\")\n",
        "\n",
        "# Load the preprocessed SMILES data\n",
        "df = pd.read_csv('/content/preprocessed_smiles.csv')\n",
        "X = df['Preprocessed_SMILES']\n",
        "\n",
        "# Split the data into a training set and a validation set\n",
        "(X_train, X_val) = train_test_split(X, test_size=0.25)\n",
        "\n",
        "# Evaluate the VAE model on the validation set\n",
        "val_loss, val_accuracy = vae.evaluate(X_val, X_val, verbose=0)\n",
        "\n",
        "# Save the validation loss and accuracy to a variable\n",
        "validation_results = {\"val_loss\": val_loss, \"val_accuracy\": val_accuracy}\n",
        "\n",
        "# Save the variable to a file\n",
        "with open(\"validation_results.json\", \"w\") as f:\n",
        "    json.dump(validation_results, f, indent=4)\n"
      ],
      "metadata": {
        "id": "Wmx9QPZLcLku"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}